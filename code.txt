docker-compose up -d

## Loading data into Postgres

docker run -it --rm --network=kafka-exercise_default \
         -v "$PWD":/home/config/ \
         postgres:13.2 psql -h postgres -U postgres

Password = postgres

At the command line:

```
##CREATE DATABASE students;
##\connect students;
```

Load our admission data table:

```
CREATE TABLE admission
(student_id INTEGER, gre INTEGER, toefl INTEGER, cpga DOUBLE PRECISION, admit_chance DOUBLE PRECISION,
CONSTRAINT student_id_pk PRIMARY KEY (student_id));

\copy admission FROM '/home/config/admit_1.csv' DELIMITER ',' CSV HEADER
```

Load the research data table with:

```
CREATE TABLE research
(student_id INTEGER, rating INTEGER, research INTEGER,
PRIMARY KEY (student_id));

\copy research FROM '/home/config/research_1.csv' DELIMITER ',' CSV HEADER
```

docker-compose exec broker kafka-topics --create \
    --bootstrap-server localhost:9092 \
    --partitions 1 \
    --replication-factor 1 \
    --topic psg-admission

## Connect Postgres database as a source to Kafka
```
curl -X POST -H "Accept:application/json" -H "Content-Type: application/json" \
      --data @connect_postgres.json http://localhost:8083/connectors

curl -X POST -H "Content-Type: application/json" \
    --data @connect_postgres.json http://localhost:8083/connectors
```

The connector 'postgres-source' should show up when curling for the list
of existing connectors:

```
curl -H "Accept:application/json" localhost:8083/connectors/
```
docker logs -f connect

The two tables in the `students` database will now show up as topics in Kafka.
You can check this by entering the Kafka container:

```
docker exec -it <kafka-container-id> /bin/bash
```

and listing the available topics:

```
/usr/bin/kafka-topics --list --zookeeper zookeeper:2181
```

```
/usr/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning

```


## Create tables in KSQL

Bring up a KSQL server command line client as a container:

```
docker run --network postgres-kafka-demo_default \
           --interactive --tty --rm \
           confluentinc/cp-ksql-cli:latest \
           http://ksql-server:8088
```

docker-compose exec ksqldb-cli ksql http://ksqldb-server:8088


To see your updates, a few settings need to be configured by first running:

```
set 'commit.interval.ms'='2000';
set 'cache.max.bytes.buffering'='10000000';
set 'auto.offset.reset'='earliest';
```


### Mirror Postgres tables

The Postgres table topics will be visible in KSQL, and we will create
KSQL streams to auto update KSQL tables mirroring the Postgres tables:

```
SHOW TOPICS;

CREATE STREAM admission_src (student_id INTEGER, gre INTEGER, toefl INTEGER, cpga DOUBLE, admit_chance DOUBLE)\
WITH (KAFKA_TOPIC='dbserver1.public.admission', VALUE_FORMAT='AVRO');

CREATE STREAM admission_src_rekey WITH (PARTITIONS=1) AS \
SELECT * FROM admission_src PARTITION BY student_id;

SHOW STREAMS;

CREATE TABLE admission (student_id INTEGER, gre INTEGER, toefl INTEGER, cpga DOUBLE, admit_chance DOUBLE)\
WITH (KAFKA_TOPIC='ADMISSION_SRC_REKEY', VALUE_FORMAT='AVRO', KEY='student_id');

SHOW TABLES;

CREATE STREAM research_src (student_id INTEGER, rating INTEGER, research INTEGER)\
WITH (KAFKA_TOPIC='dbserver1.public.research', VALUE_FORMAT='AVRO');

CREATE STREAM research_src_rekey WITH (PARTITIONS=1) AS \
SELECT * FROM research_src PARTITION BY student_id;

CREATE TABLE research (student_id INTEGER, rating INTEGER, research INTEGER)\
WITH (KAFKA_TOPIC='RESEARCH_SRC_REKEY', VALUE_FORMAT='AVRO', KEY='student_id');
```

Currently KSQL uses uppercase casing convention for stream, table, and field
names.


### Create downstream tables

We will create a new KSQL streaming table to join students' chance of
admission with research experience.

```
CREATE TABLE research_boost AS \
  SELECT a.student_id as student_id, \
         a.admit_chance as admit_chance, \
         r.research as research \
  FROM admission a \
  LEFT JOIN research r on a.student_id = r.student_id;
```

and another table calculating the average chance of admission for
students with and without research experience:

```
CREATE TABLE research_ave_boost AS \
     SELECT research, SUM(admit_chance)/COUNT(admit_chance) as ave_chance \
     FROM research_boost \
     WITH (KAFKA_TOPIC='research_ave_boost', VALUE_FORMAT='delimited', KEY='research') \
     GROUP BY research;
```

## Add a connector to sink a KSQL table back to Postgres

The postgres-sink.json configuration file will create a RESEARCH_AVE_BOOST
table and send the data back to Postgres.

```
curl -X POST -H "Accept:application/json" -H "Content-Type: application/json" \
      --data @postgres-sink.json http://localhost:8083/connectors
```

## Update the source Postgres tables and watch the Postgres sink table update

The RESEARCH_AVE_BOOST table should now be available in Postgres to query:

```
SELECT "AVE_CHANCE" FROM "RESEARCH_AVE_BOOST"
  WHERE cast("RESEARCH" as INT)=0;
```

With these data the average admission chance will be 65.19%.
